{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Wrangle Report\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This report documents the process of gathering, assessing, and cleaning data for the WeRateDogs Twitter archive. The data sources include \"twitter_archive_enhanced.csv\" and \"image_predictions.tsv,\" provided by Udacity, as well as additional data collected via the Tweepy API.\n",
    "\n",
    "## Chapter 1: Gathering the Data\n",
    "\n",
    "The data collection for the WeRateDogs Twitter archive involved three main sources:\n",
    "\n",
    "1. **Twitter Archive CSV**: The dataset \"twitter_archive_enhanced.csv\" was directly downloaded from Udacity. It includes comprehensive details about the tweets, such as tweet ID, text, rating, and the dog's age level.\n",
    "\n",
    "2. **Image Predictions TSV**: The file \"image_predictions.tsv,\" also provided by Udacity, contains predictions generated by a machine learning model about the images included in the tweets. This file offers insights into the predicted breed of each dog in the images.\n",
    "\n",
    "3. **Twitter Data via Tweepy API**: Additional data was collected using the Tweepy API. This dataset includes details such as favorite count, tweet ID, language, retweet count, and whether the tweet was a retweet.\n",
    "\n",
    "## Chapter 2: Assessing the Data\n",
    "\n",
    "The assessment phase aimed to evaluate the quality and structure of the gathered datasets to ensure their suitability for analysis. Key methods used for assessment included:\n",
    "\n",
    "1. **`.head()` Method**: This function was employed to manually inspect the first few rows of each dataset. It provided a quick visual check of the data's structure and content, helping identify obvious issues.\n",
    "\n",
    "2. **`.info()` Method**: This function was used to obtain a summary of each dataset programmatically. It included information on the number of entries, column names, data types, and non-null counts, which facilitated the identification of missing values and incorrect data types.\n",
    "\n",
    "These methods were instrumental in pinpointing key issues that required attention during the data cleaning phase.\n",
    "\n",
    "## Chapter 3: Cleaning the Data\n",
    "\n",
    "The cleaning phase involved addressing the issues identified during the assessment phase. Prior to cleaning, copies of the original datasets were made to preserve the initial data. The following methods were used to clean the data:\n",
    "\n",
    "1. **`.replace()` Method**: This method was used to correct errors and standardize data by replacing specific values with new ones.\n",
    "\n",
    "2. **`.isna()` Method**: Used to check for missing values, this method helped identify entries with missing data.\n",
    "\n",
    "3. **`.dtype()` Method**: This method was employed to verify that the data types of columns were appropriate for analysis.\n",
    "\n",
    "4. **`pd.to_datetime()` Method**: This function was used to convert columns to datetime format, ensuring that date and time data were properly formatted for analysis.\n",
    "\n",
    "5. **`.drop()` Method**: This method allowed for the removal of specific rows or columns that were deemed unnecessary or problematic.\n",
    "\n",
    "6. **`.merge()` Method**: This function combined multiple datasets into a single dataset based on a common column or index, facilitating a more comprehensive analysis.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This report outlined the process of gathering, assessing, and cleaning the WeRateDogs Twitter archive data. We collected data from three primary sources: \"twitter_archive_enhanced.csv,\" \"image_predictions.tsv,\" and the Tweepy API. \n",
    "\n",
    "Key methods for assessment included `.head()` and `.info()`, which were used to inspect and summarize the data. For cleaning, methods such as `.replace()` for corrections, `.drop()` for removing unnecessary data, and `.merge()` for combining datasets were employed.\n",
    "\n",
    "With the data now cleaned and organized, it is ready for analysis.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
